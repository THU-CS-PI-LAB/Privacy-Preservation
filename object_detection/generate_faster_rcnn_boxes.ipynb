{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.7)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 15\n",
    "source_path = \"./data/\"\n",
    "dest_path = \"./result/faster-rcnn-objects/\" + str(size) + \"/\"\n",
    "if os.path.isdir(dest_path):\n",
    "    shutil.rmtree(dest_path)\n",
    "os.mkdir(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(source_path):\n",
    "    if folder == \".DS_Store\":\n",
    "        continue\n",
    "    for picture in os.listdir(os.path.join(source_path, folder)):\n",
    "        if picture == \".DS_Store\":\n",
    "            continue\n",
    "        index = re.match(r'(\\d*)_(\\d*)x(\\d*).jpg', picture).group(1)\n",
    "        img = Image.open(os.path.join(source_path, folder, picture))\n",
    "        img = img.resize((size, size), Image.NEAREST)\n",
    "        trans_totensor = torchvision.transforms.ToTensor()\n",
    "        img = trans_totensor(img)\n",
    "        batch = [preprocess(img)]\n",
    "        prediction = model(batch)[0]\n",
    "        labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "        filename = open(dest_path + index + '.txt', 'w')\n",
    "        for p, [xmin, ymin, xmax, ymax], label in zip(prediction['scores'].tolist(), prediction['boxes'].tolist(), labels):\n",
    "            transformed_label = re.split(r' ', label)[-1]\n",
    "            xmin_transformed = xmin / size * 224\n",
    "            xmax_transformed = xmax / size * 224\n",
    "            ymin_transformed = ymin / size * 224\n",
    "            ymax_transformed = ymax / size * 224\n",
    "            filename.write(f'{transformed_label} {p:.3f} {xmin_transformed:.3f} {ymin_transformed:.3f} {xmax_transformed:.3f} {ymax_transformed:.3f}\\n')\n",
    "        filename.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de401cb0ebe5c3a98decf1b15e5a14f89ea85648ba9964f9d64ffd16b516b437"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
